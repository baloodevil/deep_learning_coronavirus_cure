{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Children's Book Author\n",
    "\n",
    "Train an RNN to make sentences for a children's book like the example in [this YouTube video](https://www.youtube.com/watch?v=WCUNPb-5EYI). Useful as a high-level \"make an RNN work\" project, but haven't gone into the details of how it works yet.\n",
    "\n",
    "Expanded on the example to show it more examples of sentences in the format \"<person 1> saw <person 2> .\" and see if it could 1) Keep the appropriate grammar (e.g. avoid \"Jane.\" or \"Jane saw Spot saw Doug saw Jane.\" and 2) make new (correct) sentences that it hasn't seen before (e.g. \"Jane saw Luke.\", which isn't in the training set).\n",
    "\n",
    "Most of the code was adapted from this [Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/text_generation), which originally genereated Shakespeare-like text on a character-by-character basis. Adapted it to use words as tokens instead of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a dataset for training, starting with a string of sentences of the form \"Jane saw Spot .\" The period is separated by a space so that it is treated as a separate word/token the RNN can choose to add.\n",
    "\n",
    "\n",
    "### Initial text string\n",
    "\n",
    "Start with a string of many example sentences. Names are split into groups, and sentences only use names within a single group. This way we can see if the RNN creates new sentences by combining names from the different groups (e.g. the training data will not contain \"Spot saw Leia .\" but the output might generate that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doug saw Jane . Doug saw Spot . Jane saw Doug . Jane saw Spot . Spot saw Doug . Spot saw Jane .\n"
     ]
    }
   ],
   "source": [
    "names = ['Doug', 'Jane', 'Spot']\n",
    "\n",
    "text_array = []\n",
    "for name_pair in permutations(names, 2):\n",
    "    text_array.append(' saw '.join(name_pair))\n",
    "\n",
    "data_text = ' . '.join(text_array) + ' .' # Need that last period\n",
    "\n",
    "print(data_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the data\n",
    "\n",
    "The dataset is made by splitting the string into a list of individual words. Scikit-Learn's `LabelEncoder` class is used to convert each word into a number, so it can be used as input into the RNN. The encoded words are stored in `dataset_enc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['.' 'Doug' 'Jane' 'Spot' 'saw']\n",
      "Orignal data: ['Doug' 'saw' 'Jane' '.' 'Doug' 'saw' 'Spot' '.']\n",
      "Encoded data: [1 4 2 0 1 4 3 0]\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array(data_text.split())\n",
    "encoder = LabelEncoder()\n",
    "dataset_enc = encoder.fit_transform(dataset)\n",
    "\n",
    "\n",
    "print(f'Vocabulary: {encoder.classes_}')\n",
    "print(f'Orignal data: {dataset[:8]}\\nEncoded data: {dataset_enc[:8]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Dataset Format\n",
    "\n",
    "Convert our encoded data (a list of numbers representing word tokens) into a format usable by the Tensorflow RNN. We want our input to be a series of number representing words (e.g. [1, 16, 14] -> \"Doug saw Spot\"), and the output should be the series shifted one word into the future (e.g. [16, 14, 0] -> \"saw Spot .\"). \n",
    "\n",
    "This converts our list of encoded numbers into a tensorflow dataset, then formats it by:\n",
    "\n",
    "- Grabbing batches 1 longer than the input length (e.g. [1, 16, 14, 0, 2] for an input length of 4)\n",
    "- Mapping the batches into input and target (e.g. input: [1, 16, 14, 0] target: [16, 14, 0, 2])\n",
    "- Shuffling the input/output pairs so it doesn't always see input in the same order during training\n",
    "    + This doesn't shuffle the words in the input/target, only the order it sees the input/target pairs\n",
    "- Set the batch size used for training (the number of input/target pairs to give to the model at each training step)\n",
    "    + Prediction needs a batch size of 1, and apparently the batch size can't be changed after creating a model. So if you use a larger batch size for training, for prediction you'd have to make a new model with a batch size of 1 and load the weights of the trained model into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Input: [1 0 2 4]\n",
      "Target: [0 2 4 3]\n",
      "Input Translated: Doug . Jane saw\n",
      "Target Translated: . Jane saw Spot\n",
      " \n",
      "Batch 1\n",
      "Input: [4 3 0 2]\n",
      "Target: [3 0 2 4]\n",
      "Input Translated: saw Spot . Jane\n",
      "Target Translated: Spot . Jane saw\n",
      " \n",
      "Batch 2\n",
      "Input: [0 3 4 1]\n",
      "Target: [3 4 1 0]\n",
      "Input Translated: . Spot saw Doug\n",
      "Target Translated: Spot saw Doug .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Input and target are sets of 4 words, with target shifted one word into the future\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "seq_length = 4 # Length of input and target strings\n",
    "batch_size = 1 # Use 1 so we don't have to rebuild model for generating data after training\n",
    "buffer_size = 4\n",
    "dataset_tf = (tf.data.Dataset.from_tensor_slices(dataset_enc) # Make tf dataset from encoded dataset\n",
    "              .batch(seq_length+1, drop_remainder=True) # Take a batch of 5 words at a time, dropping any remainder\n",
    "              .map(split_input_target) # From each 5-word batch, return input (words 1-4) and target (words 2-5)\n",
    "              .shuffle(buffer_size) # tf reads in buffer_size elements into memory and shuffles those elements\n",
    "              .batch(batch_size, drop_remainder=True)) # This is the batch size used for training\n",
    "\n",
    "for batch_num, (input_text, target_text) in enumerate(dataset_tf.take(3)):\n",
    "    print(f'Batch {batch_num}')\n",
    "    for batch_input, batch_target in zip(input_text, target_text):\n",
    "        print(f'Input: {batch_input}')\n",
    "        print(f'Target: {batch_target}')\n",
    "        print('Input Translated: ' + ' '.join(encoder.inverse_transform(batch_input)))\n",
    "        print('Target Translated: ' + ' '.join(encoder.inverse_transform(batch_target)))\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the RNN Model\n",
    "\n",
    "**Create a model with 3 layers:**\n",
    "\n",
    "* Embedding: used to convert number representations of words into one-hot vectors usable by the RNN\n",
    "   + `embedding_dim` is (part of the) size of the layer output, not sure what effect it has or if it can just be the size of the data\n",
    "* LSTM: the recurrent layer\n",
    "   + `rnn_units` is passed to `units` parameter, determines output dimensionality. Not sure of details of exactly what it does\n",
    "   + `return_sequences` parameter determines \"whether to return the the last output in the output sequence, or the full sequence.\"\n",
    "       - I believe setting it to true has it return the entire sequence (input + prediction? or current and past predictions?). Not sure what exactly this does, but it seems to need to be True to work right\n",
    "   + `stateful` parameter: \"f True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\" \n",
    "       + Something about passing information into the next step, but not sure exactly what the state is\n",
    "   \n",
    "**Create a loss function and compile**\n",
    "\n",
    "- Use the categorical crossentropy loss, but have to create our own function so we can set `from_logits` parameter to be true\n",
    "    + Logits are an inverse of the sigmoid function, limiting the x-axis to the [0,1] range (or probably [-1,1])\n",
    "    + Haven't figured out why we're using them yet, or for what\n",
    "- Use Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0411 13:55:12.370067 19296 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.LSTM object at 0x0000025CBD8831D0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (1, None, 1024)           5120      \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (1, None, 256)            1311744   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (1, None, 5)              1285      \n",
      "=================================================================\n",
      "Total params: 1,318,149\n",
      "Trainable params: 1,318,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create RNN\n",
    "\n",
    "vocab_size = len(encoder.classes_)\n",
    "\n",
    "# Embedding dimension\n",
    "embedding_dim = 1024\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 256\n",
    "\n",
    "logdir = \"logs/scalars/\" + str(embedding_dim) + \"/\" + str(rnn_units) + \"/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # Embedding layer maps words to vectors\n",
    "    tf.keras.layers.Embedding(vocab_size, \n",
    "                              embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    \n",
    "    # Recurrent layer\n",
    "    tf.keras.layers.LSTM(units=rnn_units, \n",
    "                         return_sequences=True, \n",
    "                         stateful=True),\n",
    "    \n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "# Use categorical crossentropy as loss function, use custom function so from_logits is true\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# Compile with loss function and adam optimizer\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "- Set the number of epochs and train the model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4==============================] - 3s 741ms/step - loss: 1.6158\n",
      "Epoch 2/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 1.4976\n",
      "Epoch 3/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 1.4078\n",
      "Epoch 4/50\n",
      "4/4==============================] - 0s 43ms/step - loss: 1.3063\n",
      "Epoch 5/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 1.1858\n",
      "Epoch 6/50\n",
      "4/4==============================] - 0s 45ms/step - loss: 1.0639\n",
      "Epoch 7/50\n",
      "4/4==============================] - 0s 45ms/step - loss: 0.9632\n",
      "Epoch 8/50\n",
      "4/4==============================] - 0s 43ms/step - loss: 0.8355\n",
      "Epoch 9/50\n",
      "4/4==============================] - 0s 43ms/step - loss: 0.6673\n",
      "Epoch 10/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.5163\n",
      "Epoch 11/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.4051\n",
      "Epoch 12/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.2919\n",
      "Epoch 13/50\n",
      "4/4==============================] - 0s 46ms/step - loss: 0.1745\n",
      "Epoch 14/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.1053\n",
      "Epoch 15/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.0619\n",
      "Epoch 16/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.0340\n",
      "Epoch 17/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.0196\n",
      "Epoch 18/50\n",
      "4/4==============================] - 0s 43ms/step - loss: 0.0126\n",
      "Epoch 19/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.0089\n",
      "Epoch 20/50\n",
      "4/4==============================] - 0s 43ms/step - loss: 0.0067\n",
      "Epoch 21/50\n",
      "4/4==============================] - 0s 40ms/step - loss: 0.0054\n",
      "Epoch 22/50\n",
      "4/4==============================] - 0s 41ms/step - loss: 0.0045\n",
      "Epoch 23/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.0038\n",
      "Epoch 24/50\n",
      "4/4==============================] - 0s 45ms/step - loss: 0.0034\n",
      "Epoch 25/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.0030\n",
      "Epoch 26/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.0028\n",
      "Epoch 27/50\n",
      "4/4==============================] - 0s 40ms/step - loss: 0.0025\n",
      "Epoch 28/50\n",
      "4/4==============================] - 0s 44ms/step - loss: 0.0024\n",
      "Epoch 29/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.0022\n",
      "Epoch 30/50\n",
      "4/4==============================] - 0s 46ms/step - loss: 0.0021\n",
      "Epoch 31/50\n",
      "4/4==============================] - 0s 43ms/step - loss: 0.0019\n",
      "Epoch 32/50\n",
      "4/4==============================] - 0s 44ms/step - loss: 0.0018\n",
      "Epoch 33/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.0017\n",
      "Epoch 34/50\n",
      "4/4==============================] - 0s 46ms/step - loss: 0.0017\n",
      "Epoch 35/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.0016\n",
      "Epoch 36/50\n",
      "4/4==============================] - 0s 41ms/step - loss: 0.0015\n",
      "Epoch 37/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.0014\n",
      "Epoch 38/50\n",
      "4/4==============================] - 0s 41ms/step - loss: 0.0014\n",
      "Epoch 39/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 0.0013\n",
      "Epoch 40/50\n",
      "4/4==============================] - 0s 41ms/step - loss: 0.0013\n",
      "Epoch 41/50\n",
      "4/4==============================] - 0s 43ms/step - loss: 0.0012\n",
      "Epoch 42/50\n",
      "4/4==============================] - 0s 43ms/step - loss: 0.0012\n",
      "Epoch 43/50\n",
      "4/4==============================] - 0s 44ms/step - loss: 0.0011\n",
      "Epoch 44/50\n",
      "4/4==============================] - 0s 41ms/step - loss: 0.0011\n",
      "Epoch 45/50\n",
      "4/4==============================] - 0s 43ms/step - loss: 0.0011\n",
      "Epoch 46/50\n",
      "4/4==============================] - 0s 43ms/step - loss: 0.0010\n",
      "Epoch 47/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 9.8096e-04\n",
      "Epoch 48/50\n",
      "4/4==============================] - 0s 44ms/step - loss: 9.4923e-04\n",
      "Epoch 49/50\n",
      "4/4==============================] - 0s 42ms/step - loss: 9.1913e-04\n",
      "Epoch 50/50\n",
      "4/4==============================] - 0s 45ms/step - loss: 8.9054e-04\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "history = model.fit(dataset_tf, epochs=epochs, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Start by seeding it with the sentence \"Jane saw Doug .\" and having it generate new text word by word. \n",
    "\n",
    "We choose the new word by sampling from the output predictions, rather than simply taking the highest probability word. Apparently always taking the most likely word can get it stuck in a loop?\n",
    "\n",
    "The input for the next step is the last 3 words from the previous input, followed by the newest prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jane saw Spot . Doug saw Jane . Doug saw Jane . Doug . Jane saw Spot . Jane saw\n"
     ]
    }
   ],
   "source": [
    "input_text = encoder.transform('Jane saw Spot .'.split())\n",
    "input_text = tf.expand_dims(input_text, 0) # Some kind of formatting for tensorflow\n",
    "\n",
    "generated_text = []\n",
    "\n",
    "# No idea where the term comes from, but low values give more predictable results, high values more surprising\n",
    "temperature = 1\n",
    "\n",
    "model.reset_states() # ? Does this drop memory of the recently seen text?\n",
    "\n",
    "for i in range(20):\n",
    "    predictions = model(input_text)\n",
    "    \n",
    "    # Remove batch dimension\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    \n",
    "    # Sample from the output predictions instead of taking the argmax\n",
    "    # Apparently argmax can get it stuck in a loop\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "    \n",
    "    # Add predicted value to the input and drop the oldest value\n",
    "    input_text = np.append(np.array(input_text)[0, 1:], [predicted_id])\n",
    "    input_text = tf.expand_dims(input_text, 0)\n",
    "    \n",
    "    generated_text.extend(encoder.inverse_transform([predicted_id]))\n",
    "    \n",
    "print(\" \".join(generated_text))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "It sort of works! It generates words, usually into sensible sentences. It will occasionally cross words between groups (e.g. \"Hermione saw Merry .\" or \"Kaylee saw Han .\". It will also sometimes do weird things, like use two periods in a row. Or \"Luigi saw Luigi .\" or \"Ron saw Harry saw Ron .\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
