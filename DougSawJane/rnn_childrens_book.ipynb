{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Children's Book Author\n",
    "\n",
    "Train an RNN to make sentences for a children's book like the example in [this YouTube video](https://www.youtube.com/watch?v=WCUNPb-5EYI). Useful as a high-level \"make an RNN work\" project, but haven't gone into the details of how it works yet.\n",
    "\n",
    "Expanded on the example to show it more examples of sentences in the format \"<person 1> saw <person 2> .\" and see if it could 1) Keep the appropriate grammar (e.g. avoid \"Jane.\" or \"Jane saw Spot saw Doug saw Jane.\" and 2) make new (correct) sentences that it hasn't seen before (e.g. \"Jane saw Luke.\", which isn't in the training set).\n",
    "\n",
    "Most of the code was adapted from this [Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/text_generation), which originally genereated Shakespeare-like text on a character-by-character basis. Adapted it to use words as tokens instead of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a dataset for training, starting with a string of sentences of the form \"Jane saw Spot .\" The period is separated by a space so that it is treated as a separate word/token the RNN can choose to add.\n",
    "\n",
    "\n",
    "### Initial text string\n",
    "\n",
    "Start with a string of many example sentences. Names are split into groups, and sentences only use names within a single group. This way we can see if the RNN creates new sentences by combining names from the different groups (e.g. the training data will not contain \"Spot saw Leia .\" but the output might generate that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doug saw Jane . Doug saw Spot . Doug saw Kaylee . Doug saw Mal . Doug saw Link . Doug saw Zelda . Doug saw Mario . Doug saw Luigi . Jane saw Doug . Jane saw Spot . Jane saw Kaylee . Jane saw Mal . Jane saw Link . Jane saw Zelda . Jane saw Mario . Jane saw Luigi . Spot saw Doug . Spot saw Jane . Spot saw Kaylee . Spot saw Mal . Spot saw Link . Spot saw Zelda . Spot saw Mario . Spot saw Luigi . Kaylee saw Doug . Kaylee saw Jane . Kaylee saw Spot . Kaylee saw Mal . Kaylee saw Link . Kaylee saw Zelda . Kaylee saw Mario . Kaylee saw Luigi . Mal saw Doug . Mal saw Jane . Mal saw Spot . Mal saw Kaylee . Mal saw Link . Mal saw Zelda . Mal saw Mario . Mal saw Luigi . Link saw Doug . Link saw Jane . Link saw Spot . Link saw Kaylee . Link saw Mal . Link saw Zelda . Link saw Mario . Link saw Luigi . Zelda saw Doug . Zelda saw Jane . Zelda saw Spot . Zelda saw Kaylee . Zelda saw Mal . Zelda saw Link . Zelda saw Mario . Zelda saw Luigi . Mario saw Doug . Mario saw Jane . Mario saw Spot . Mario saw Kaylee . Mario saw Mal . Mario saw Link . Mario saw Zelda . Mario saw Luigi . Luigi saw Doug . Luigi saw Jane . Luigi saw Spot . Luigi saw Kaylee . Luigi saw Mal . Luigi saw Link . Luigi saw Zelda . Luigi saw Mario . Leia saw Luke . Leia saw Han . Leia saw Harry . Leia saw Hermione . Leia saw Ron . Luke saw Leia . Luke saw Han . Luke saw Harry . Luke saw Hermione . Luke saw Ron . Han saw Leia . Han saw Luke . Han saw Harry . Han saw Hermione . Han saw Ron . Harry saw Leia . Harry saw Luke . Harry saw Han . Harry saw Hermione . Harry saw Ron . Hermione saw Leia . Hermione saw Luke . Hermione saw Han . Hermione saw Harry . Hermione saw Ron . Ron saw Leia . Ron saw Luke . Ron saw Han . Ron saw Harry . Ron saw Hermione . Frodo saw Sam . Frodo saw Merry . Frodo saw Pippin . Sam saw Frodo . Sam saw Merry . Sam saw Pippin . Merry saw Frodo . Merry saw Sam . Merry saw Pippin . Pippin saw Frodo . Pippin saw Sam . Pippin saw Merry .\n"
     ]
    }
   ],
   "source": [
    "names1 = ['Doug', 'Jane', 'Spot', 'Kaylee', 'Mal', 'Link', 'Zelda', 'Mario', 'Luigi']\n",
    "names2 = ['Leia', 'Luke', 'Han', 'Harry', 'Hermione', 'Ron']\n",
    "names3 = ['Frodo', 'Sam', 'Merry', 'Pippin']\n",
    "\n",
    "text_array = []\n",
    "for name_pair in permutations(names1, 2):\n",
    "    text_array.append(' saw '.join(name_pair))\n",
    "for name_pair in permutations(names2, 2):\n",
    "    text_array.append(' saw '.join(name_pair))\n",
    "for name_pair in permutations(names3, 2):\n",
    "    text_array.append(' saw '.join(name_pair))\n",
    "data_text = ' . '.join(text_array) + ' .' # Need that last period\n",
    "\n",
    "print(data_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the data\n",
    "\n",
    "The dataset is made by splitting the string into a list of individual words. Scikit-Learn's `LabelEncoder` class is used to convert each word into a number, so it can be used as input into the RNN. The encoded words are stored in `dataset_enc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['.' 'Doug' 'Frodo' 'Han' 'Harry' 'Hermione' 'Jane' 'Kaylee' 'Leia' 'Link'\n",
      " 'Luigi' 'Luke' 'Mal' 'Mario' 'Merry' 'Pippin' 'Ron' 'Sam' 'Spot' 'Zelda'\n",
      " 'saw']\n",
      "Orignal data: ['Doug' 'saw' 'Jane' '.' 'Doug' 'saw' 'Spot' '.']\n",
      "Encoded data: [ 1 20  6  0  1 20 18  0]\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array(data_text.split())\n",
    "encoder = LabelEncoder()\n",
    "dataset_enc = encoder.fit_transform(dataset) # Reshape dataset to be a single column vector\n",
    "\n",
    "\n",
    "print(f'Vocabulary: {encoder.classes_}')\n",
    "print(f'Orignal data: {dataset[:8]}\\nEncoded data: {dataset_enc[:8]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Dataset Format\n",
    "\n",
    "Convert our encoded data (a list of numbers representing word tokens) into a format usable by the Tensorflow RNN. We want our input to be a series of number representing words (e.g. [1, 16, 14] -> \"Doug saw Spot\"), and the output should be the series shifted one word into the future (e.g. [16, 14, 0] -> \"saw Spot .\"). \n",
    "\n",
    "This converts our list of encoded numbers into a tensorflow dataset, then formats it by:\n",
    "\n",
    "- Grabbing batches 1 longer than the input length (e.g. [1, 16, 14, 0, 2] for an input length of 4)\n",
    "- Mapping the batches into input and target (e.g. input: [1, 16, 14, 0] target: [16, 14, 0, 2])\n",
    "- Shuffling the input/output pairs so it doesn't always see input in the same order during training\n",
    "    + This doesn't shuffle the words in the input/target, only the order it sees the input/target pairs\n",
    "- Set the batch size used for training (the number of input/target pairs to give to the model at each training step)\n",
    "    + Prediction needs a batch size of 1, and apparently the batch size can't be changed after creating a model. So if you use a larger batch size for training, for prediction you'd have to make a new model with a batch size of 1 and load the weights of the trained model into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Input: [ 0  1 20  9]\n",
      "Target: [ 1 20  9  0]\n",
      "Input Translated: . Doug saw Link\n",
      "Target Translated: Doug saw Link .\n",
      " \n",
      "Batch 1\n",
      "Input: [20 18  0  1]\n",
      "Target: [18  0  1 20]\n",
      "Input Translated: saw Spot . Doug\n",
      "Target Translated: Spot . Doug saw\n",
      " \n",
      "Batch 2\n",
      "Input: [ 1 20 19  0]\n",
      "Target: [20 19  0  1]\n",
      "Input Translated: Doug saw Zelda .\n",
      "Target Translated: saw Zelda . Doug\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Input and target are sets of 4 words, with target shifted one word into the future\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "seq_length = 4 # Length of input and target strings\n",
    "batch_size = 1 # Use 1 so we don't have to rebuild model for generating data after training\n",
    "buffer_size = 4\n",
    "dataset_tf = (tf.data.Dataset.from_tensor_slices(dataset_enc) # Make tf dataset from encoded dataset\n",
    "              .batch(seq_length+1, drop_remainder=True) # Take a batch of 5 words at a time, dropping any remainder\n",
    "              .map(split_input_target) # From each 5-word batch, return input (words 1-4) and target (words 2-5)\n",
    "              .shuffle(buffer_size) # tf reads in buffer_size elements into memory and shuffles those elements\n",
    "              .batch(batch_size, drop_remainder=True)) # This is the batch size used for training\n",
    "\n",
    "for batch_num, (input_text, target_text) in enumerate(dataset_tf.take(3)):\n",
    "    print(f'Batch {batch_num}')\n",
    "    for batch_input, batch_target in zip(input_text, target_text):\n",
    "        print(f'Input: {batch_input}')\n",
    "        print(f'Target: {batch_target}')\n",
    "        print('Input Translated: ' + ' '.join(encoder.inverse_transform(batch_input)))\n",
    "        print('Target Translated: ' + ' '.join(encoder.inverse_transform(batch_target)))\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the RNN Model\n",
    "\n",
    "**Create a model with 3 layers:**\n",
    "\n",
    "* Embedding: used to convert number representations of words into one-hot vectors usable by the RNN\n",
    "   + `embedding_dim` is (part of the) size of the layer output, not sure what effect it has or if it can just be the size of the data\n",
    "* LSTM: the recurrent layer\n",
    "   + `rnn_units` is passed to `units` parameter, determines output dimensionality. Not sure of details of exactly what it does\n",
    "   + `return_sequences` parameter determines \"whether to return the the last output in the output sequence, or the full sequence.\"\n",
    "       - I believe setting it to true has it return the entire sequence (input + prediction? or current and past predictions?). Not sure what exactly this does, but it seems to need to be True to work right\n",
    "   + `stateful` parameter: \"f True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\" \n",
    "       + Something about passing information into the next step, but not sure exactly what the state is\n",
    "   \n",
    "**Create a loss function and compile**\n",
    "\n",
    "- Use the categorical crossentropy loss, but have to create our own function so we can set `from_logits` parameter to be true\n",
    "    + Logits are an inverse of the sigmoid function, limiting the x-axis to the [0,1] range (or probably [-1,1])\n",
    "    + Haven't figured out why we're using them yet, or for what\n",
    "- Use Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (1, None, 256)            5376      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (1, None, 21)             21525     \n",
      "=================================================================\n",
      "Total params: 5,273,877\n",
      "Trainable params: 5,273,877\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create RNN\n",
    "\n",
    "vocab_size = len(encoder.classes_)\n",
    "\n",
    "# Embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # Embedding layer maps words to vectors\n",
    "    tf.keras.layers.Embedding(vocab_size, \n",
    "                              embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    \n",
    "    # Recurrent layer\n",
    "    tf.keras.layers.LSTM(units=rnn_units, \n",
    "                         return_sequences=True, \n",
    "                         stateful=True),\n",
    "    \n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "# Use categorical crossentropy as loss function, use custom function so from_logits is true\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# Compile with loss function and adam optimizer\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "- Set the number of epochs and train the model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 91 steps\n",
      "Epoch 1/10\n",
      "91/91 [==============================] - 6s 63ms/step - loss: 2.4654\n",
      "Epoch 2/10\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.6406\n",
      "Epoch 3/10\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.5890\n",
      "Epoch 4/10\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.5789\n",
      "Epoch 5/10\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 1.6216\n",
      "Epoch 6/10\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 1.5836\n",
      "Epoch 7/10\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 1.5341\n",
      "Epoch 8/10\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.5389\n",
      "Epoch 9/10\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 1.4212\n",
      "Epoch 10/10\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.4033\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(dataset_tf, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Start by seeding it with the sentence \"Jane saw Doug .\" and having it generate new text word by word. \n",
    "\n",
    "We choose the new word by sampling from the output predictions, rather than simply taking the highest probability word. Apparently always taking the most likely word can get it stuck in a loop?\n",
    "\n",
    "The input for the next step is the last 3 words from the previous input, followed by the newest prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link saw Mario saw Spot . Spot saw Spot . Luigi saw Ron . Link saw Kaylee . Luigi saw Luigi . Link saw Luigi . Doug saw Jane . Jane saw Luigi . Luigi saw Link . Mario saw Mal . . Luigi saw Mal . Luigi saw Han . Mal saw Han . . Hermione saw Merry . Merry saw Harry . Hermione saw Luke . Frodo . Pippin saw Harry . Ron saw Han . Hermione saw Pippin . Hermione saw Luke . Kaylee saw Han . Hermione saw Hermione . Ron saw Harry saw Ron .\n"
     ]
    }
   ],
   "source": [
    "input_text = encoder.transform('Jane saw Doug .'.split())\n",
    "input_text = tf.expand_dims(input_text, 0) # Some kind of formatting for tensorflow\n",
    "\n",
    "generated_text = []\n",
    "\n",
    "# No idea where the term comes from, but low values give more predictable results, high values more surprising\n",
    "temperature = 1.0\n",
    "\n",
    "model.reset_states() # ? Does this drop memory of the recently seen text?\n",
    "\n",
    "for i in range(100):\n",
    "    predictions = model(input_text)\n",
    "    \n",
    "    # Remove batch dimension\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    \n",
    "    # Sample from the output predictions instead of taking the argmax\n",
    "    # Apparently argmax can get it stuck in a loop\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "    \n",
    "    # Add predicted value to the input and drop the oldest value\n",
    "    input_text = np.append(np.array(input_text)[0, 1:], [predicted_id])\n",
    "    input_text = tf.expand_dims(input_text, 0)\n",
    "    \n",
    "    generated_text.extend(encoder.inverse_transform([predicted_id]))\n",
    "    \n",
    "print(\" \".join(generated_text))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "It sort of works! It generates words, usually into sensible sentences. It will occasionally cross words between groups (e.g. \"Hermione saw Merry .\" or \"Kaylee saw Han .\". It will also sometimes do weird things, like use two periods in a row. Or \"Luigi saw Luigi .\" or \"Ron saw Harry saw Ron .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
