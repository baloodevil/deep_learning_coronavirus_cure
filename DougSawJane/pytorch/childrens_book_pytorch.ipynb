{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Childrens Book (Pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pytorch Version of Alyssa's Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seq_length = 4 # Length of input and target strings\n",
    "batch_size = 1 # Use 1 so we don't have to rebuild model for generating data after training\n",
    "buffer_size = 4\n",
    "n_epochs = 10\n",
    "embedding_dims = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doug saw Jane . Doug saw Spot . Doug saw Kaylee . Doug saw Mal . Doug saw Link . Doug saw Zelda . Doug saw Mario . Doug saw Luigi . Jane saw Doug . Jane saw Spot . Jane saw Kaylee . Jane saw Mal . Jane saw Link . Jane saw Zelda . Jane saw Mario . Jane saw Luigi . Spot saw Doug . Spot saw Jane . Spot saw Kaylee . Spot saw Mal . Spot saw Link . Spot saw Zelda . Spot saw Mario . Spot saw Luigi . Kaylee saw Doug . Kaylee saw Jane . Kaylee saw Spot . Kaylee saw Mal . Kaylee saw Link . Kaylee saw Zelda . Kaylee saw Mario . Kaylee saw Luigi . Mal saw Doug . Mal saw Jane . Mal saw Spot . Mal saw Kaylee . Mal saw Link . Mal saw Zelda . Mal saw Mario . Mal saw Luigi . Link saw Doug . Link saw Jane . Link saw Spot . Link saw Kaylee . Link saw Mal . Link saw Zelda . Link saw Mario . Link saw Luigi . Zelda saw Doug . Zelda saw Jane . Zelda saw Spot . Zelda saw Kaylee . Zelda saw Mal . Zelda saw Link . Zelda saw Mario . Zelda saw Luigi . Mario saw Doug . Mario saw Jane . Mario saw Spot . Mario saw Kaylee . Mario saw Mal . Mario saw Link . Mario saw Zelda . Mario saw Luigi . Luigi saw Doug . Luigi saw Jane . Luigi saw Spot . Luigi saw Kaylee . Luigi saw Mal . Luigi saw Link . Luigi saw Zelda . Luigi saw Mario . Leia saw Luke . Leia saw Han . Leia saw Harry . Leia saw Hermione . Leia saw Ron . Luke saw Leia . Luke saw Han . Luke saw Harry . Luke saw Hermione . Luke saw Ron . Han saw Leia . Han saw Luke . Han saw Harry . Han saw Hermione . Han saw Ron . Harry saw Leia . Harry saw Luke . Harry saw Han . Harry saw Hermione . Harry saw Ron . Hermione saw Leia . Hermione saw Luke . Hermione saw Han . Hermione saw Harry . Hermione saw Ron . Ron saw Leia . Ron saw Luke . Ron saw Han . Ron saw Harry . Ron saw Hermione . Frodo saw Sam . Frodo saw Merry . Frodo saw Pippin . Sam saw Frodo . Sam saw Merry . Sam saw Pippin . Merry saw Frodo . Merry saw Sam . Merry saw Pippin . Pippin saw Frodo . Pippin saw Sam . Pippin saw Merry .\n"
     ]
    }
   ],
   "source": [
    "names1 = ['Doug', 'Jane', 'Spot', 'Kaylee', 'Mal', 'Link', 'Zelda', 'Mario', 'Luigi']\n",
    "names2 = ['Leia', 'Luke', 'Han', 'Harry', 'Hermione', 'Ron']\n",
    "names3 = ['Frodo', 'Sam', 'Merry', 'Pippin']\n",
    "\n",
    "text_array = []\n",
    "for name_pair in permutations(names1, 2):\n",
    "    text_array.append(' saw '.join(name_pair))\n",
    "for name_pair in permutations(names2, 2):\n",
    "    text_array.append(' saw '.join(name_pair))\n",
    "for name_pair in permutations(names3, 2):\n",
    "    text_array.append(' saw '.join(name_pair))\n",
    "data_text = ' . '.join(text_array) + ' .' # Need that last period\n",
    "\n",
    "print(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['.' 'Doug' 'Frodo' 'Han' 'Harry' 'Hermione' 'Jane' 'Kaylee' 'Leia' 'Link'\n",
      " 'Luigi' 'Luke' 'Mal' 'Mario' 'Merry' 'Pippin' 'Ron' 'Sam' 'Spot' 'Zelda'\n",
      " 'saw']\n",
      "Orignal data: ['Doug' 'saw' 'Jane' '.' 'Doug' 'saw' 'Spot' '.']\n",
      "Encoded data: [ 1 20  6  0  1 20 18  0]\n"
     ]
    }
   ],
   "source": [
    "dataset_vocab = np.array(data_text.split())\n",
    "encoder = LabelEncoder()\n",
    "dataset_enc = encoder.fit_transform(dataset_vocab) # Reshape dataset to be a single column vector\n",
    "\n",
    "print(f'Vocabulary: {encoder.classes_}')\n",
    "print(f'Orignal data: {dataset_vocab[:8]}\\nEncoded data: {dataset_enc[:8]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 20,  6,  0,  1, 20, 18,  0,  1, 20,  7,  0,  1, 20, 12,  0,  1,\n",
       "       20,  9,  0,  1, 20, 19,  0,  1, 20, 13,  0,  1, 20, 10,  0,  6, 20,\n",
       "        1,  0,  6, 20, 18,  0,  6, 20,  7,  0,  6, 20, 12,  0,  6, 20,  9,\n",
       "        0,  6, 20, 19,  0,  6, 20, 13,  0,  6, 20, 10,  0, 18, 20,  1,  0,\n",
       "       18, 20,  6,  0, 18, 20,  7,  0, 18, 20, 12,  0, 18, 20,  9,  0, 18,\n",
       "       20, 19,  0, 18, 20, 13,  0, 18, 20, 10,  0,  7, 20,  1,  0,  7, 20,\n",
       "        6,  0,  7, 20, 18,  0,  7, 20, 12,  0,  7, 20,  9,  0,  7, 20, 19,\n",
       "        0,  7, 20, 13,  0,  7, 20, 10,  0, 12, 20,  1,  0, 12, 20,  6,  0,\n",
       "       12, 20, 18,  0, 12, 20,  7,  0, 12, 20,  9,  0, 12, 20, 19,  0, 12,\n",
       "       20, 13,  0, 12, 20, 10,  0,  9, 20,  1,  0,  9, 20,  6,  0,  9, 20,\n",
       "       18,  0,  9, 20,  7,  0,  9, 20, 12,  0,  9, 20, 19,  0,  9, 20, 13,\n",
       "        0,  9, 20, 10,  0, 19, 20,  1,  0, 19, 20,  6,  0, 19, 20, 18,  0,\n",
       "       19, 20,  7,  0, 19, 20, 12,  0, 19, 20,  9,  0, 19, 20, 13,  0, 19,\n",
       "       20, 10,  0, 13, 20,  1,  0, 13, 20,  6,  0, 13, 20, 18,  0, 13, 20,\n",
       "        7,  0, 13, 20, 12,  0, 13, 20,  9,  0, 13, 20, 19,  0, 13, 20, 10,\n",
       "        0, 10, 20,  1,  0, 10, 20,  6,  0, 10, 20, 18,  0, 10, 20,  7,  0,\n",
       "       10, 20, 12,  0, 10, 20,  9,  0, 10, 20, 19,  0, 10, 20, 13,  0,  8,\n",
       "       20, 11,  0,  8, 20,  3,  0,  8, 20,  4,  0,  8, 20,  5,  0,  8, 20,\n",
       "       16,  0, 11, 20,  8,  0, 11, 20,  3,  0, 11, 20,  4,  0, 11, 20,  5,\n",
       "        0, 11, 20, 16,  0,  3, 20,  8,  0,  3, 20, 11,  0,  3, 20,  4,  0,\n",
       "        3, 20,  5,  0,  3, 20, 16,  0,  4, 20,  8,  0,  4, 20, 11,  0,  4,\n",
       "       20,  3,  0,  4, 20,  5,  0,  4, 20, 16,  0,  5, 20,  8,  0,  5, 20,\n",
       "       11,  0,  5, 20,  3,  0,  5, 20,  4,  0,  5, 20, 16,  0, 16, 20,  8,\n",
       "        0, 16, 20, 11,  0, 16, 20,  3,  0, 16, 20,  4,  0, 16, 20,  5,  0,\n",
       "        2, 20, 17,  0,  2, 20, 14,  0,  2, 20, 15,  0, 17, 20,  2,  0, 17,\n",
       "       20, 14,  0, 17, 20, 15,  0, 14, 20,  2,  0, 14, 20, 17,  0, 14, 20,\n",
       "       15,  0, 15, 20,  2,  0, 15, 20, 17,  0, 15, 20, 14,  0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes the datasets and data loaders\n",
    "class ReadFromArray(Dataset):\n",
    "\n",
    "    def __init__(self, array_enc, transform=None):\n",
    "        self.array_enc = array_enc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.array_enc) - seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.array_enc[idx:idx+seq_length]\n",
    "        target_text = self.array_enc[idx + 1 :idx+seq_length + 1]        \n",
    "        sample = {'input': input_text, 'target': target_text}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReadFromArray(dataset_enc)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(len(encoder.classes_), embedding_dims)\n",
    "        self.lstm = nn.LSTM(embedding_dims, rnn_units)\n",
    "        self.dens = nn.Linear(rnn_units, len(encoder.classes_))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.word_embeddings(x)\n",
    "        x, y = self.lstm(x)\n",
    "        x = self.dens(x)\n",
    "#         x = nn.Softmax(x)\n",
    "        return x\n",
    "\n",
    "rnn = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     # Embedding layer maps words to vectors\n",
    "#     tf.keras.layers.Embedding(vocab_size, \n",
    "#                               embedding_dim, \n",
    "#                               batch_input_shape=[batch_size, None]),\n",
    "    \n",
    "#     # Recurrent layer\n",
    "#     tf.keras.layers.LSTM(units=rnn_units, \n",
    "#                          return_sequences=True, \n",
    "#                          stateful=True),\n",
    "    \n",
    "#     # Output layer\n",
    "#     tf.keras.layers.Dense(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  9, 15, 14]])\n",
      "tensor([[18, 20, 19,  0]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-189745df00a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deep_learning_coronavirus_cure-pb67QGPc/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deep_learning_coronavirus_cure-pb67QGPc/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m    916\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deep_learning_coronavirus_cure-pb67QGPc/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deep_learning_coronavirus_cure-pb67QGPc/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        output = rnn(data['input'])\n",
    "        \n",
    "        new_output = []\n",
    "        for out in output.data.numpy()[0]:\n",
    "            new_output.append(np.where(out == np.amax(out)))\n",
    "    \n",
    "        \n",
    "        new_output = np.squeeze(np.array(new_output))\n",
    "        new_output = np.array([new_output])\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        print(torch.from_numpy(new_output))\n",
    "        print(data['target'])\n",
    "        loss = criterion(torch.from_numpy(new_output), data['target'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sam', 'Han', 'Hermione', 'Pippin'], dtype='<U8')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.inverse_transform(new_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
