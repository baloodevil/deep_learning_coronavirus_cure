{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Childrens Book (Pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pytorch Version of Alyssa's Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doug saw Jane . Doug saw Spot . Doug saw Kaylee . Doug saw Mal . Doug saw Link . Doug saw Zelda . Doug saw Mario . Doug saw Luigi . Jane saw Doug . Jane saw Spot . Jane saw Kaylee . Jane saw Mal . Jane saw Link . Jane saw Zelda . Jane saw Mario . Jane saw Luigi . Spot saw Doug . Spot saw Jane . Spot saw Kaylee . Spot saw Mal . Spot saw Link . Spot saw Zelda . Spot saw Mario . Spot saw Luigi . Kaylee saw Doug . Kaylee saw Jane . Kaylee saw Spot . Kaylee saw Mal . Kaylee saw Link . Kaylee saw Zelda . Kaylee saw Mario . Kaylee saw Luigi . Mal saw Doug . Mal saw Jane . Mal saw Spot . Mal saw Kaylee . Mal saw Link . Mal saw Zelda . Mal saw Mario . Mal saw Luigi . Link saw Doug . Link saw Jane . Link saw Spot . Link saw Kaylee . Link saw Mal . Link saw Zelda . Link saw Mario . Link saw Luigi . Zelda saw Doug . Zelda saw Jane . Zelda saw Spot . Zelda saw Kaylee . Zelda saw Mal . Zelda saw Link . Zelda saw Mario . Zelda saw Luigi . Mario saw Doug . Mario saw Jane . Mario saw Spot . Mario saw Kaylee . Mario saw Mal . Mario saw Link . Mario saw Zelda . Mario saw Luigi . Luigi saw Doug . Luigi saw Jane . Luigi saw Spot . Luigi saw Kaylee . Luigi saw Mal . Luigi saw Link . Luigi saw Zelda . Luigi saw Mario . Leia saw Luke . Leia saw Han . Leia saw Harry . Leia saw Hermione . Leia saw Ron . Luke saw Leia . Luke saw Han . Luke saw Harry . Luke saw Hermione . Luke saw Ron . Han saw Leia . Han saw Luke . Han saw Harry . Han saw Hermione . Han saw Ron . Harry saw Leia . Harry saw Luke . Harry saw Han . Harry saw Hermione . Harry saw Ron . Hermione saw Leia . Hermione saw Luke . Hermione saw Han . Hermione saw Harry . Hermione saw Ron . Ron saw Leia . Ron saw Luke . Ron saw Han . Ron saw Harry . Ron saw Hermione . Frodo saw Sam . Frodo saw Merry . Frodo saw Pippin . Sam saw Frodo . Sam saw Merry . Sam saw Pippin . Merry saw Frodo . Merry saw Sam . Merry saw Pippin . Pippin saw Frodo . Pippin saw Sam . Pippin saw Merry .\n"
     ]
    }
   ],
   "source": [
    "names1 = ['Doug', 'Jane', 'Spot', 'Kaylee', 'Mal', 'Link', 'Zelda', 'Mario', 'Luigi']\n",
    "names2 = ['Leia', 'Luke', 'Han', 'Harry', 'Hermione', 'Ron']\n",
    "names3 = ['Frodo', 'Sam', 'Merry', 'Pippin']\n",
    "\n",
    "text_array = []\n",
    "for name_pair in permutations(names1, 2):\n",
    "    text_array.append(' saw '.join(name_pair))\n",
    "for name_pair in permutations(names2, 2):\n",
    "    text_array.append(' saw '.join(name_pair))\n",
    "for name_pair in permutations(names3, 2):\n",
    "    text_array.append(' saw '.join(name_pair))\n",
    "data_text = ' . '.join(text_array) + ' .' # Need that last period\n",
    "\n",
    "print(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['.' 'Doug' 'Frodo' 'Han' 'Harry' 'Hermione' 'Jane' 'Kaylee' 'Leia' 'Link'\n",
      " 'Luigi' 'Luke' 'Mal' 'Mario' 'Merry' 'Pippin' 'Ron' 'Sam' 'Spot' 'Zelda'\n",
      " 'saw']\n",
      "Orignal data: ['Doug' 'saw' 'Jane' '.' 'Doug' 'saw' 'Spot' '.']\n",
      "Encoded data: [ 1 20  6  0  1 20 18  0]\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array(data_text.split())\n",
    "encoder = LabelEncoder()\n",
    "dataset_enc = encoder.fit_transform(dataset) # Reshape dataset to be a single column vector\n",
    "\n",
    "\n",
    "print(f'Vocabulary: {encoder.classes_}')\n",
    "print(f'Orignal data: {dataset[:8]}\\nEncoded data: {dataset_enc[:8]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 20,  6,  0,  1, 20, 18,  0,  1, 20,  7,  0,  1, 20, 12,  0,  1,\n",
       "       20,  9,  0,  1, 20, 19,  0,  1, 20, 13,  0,  1, 20, 10,  0,  6, 20,\n",
       "        1,  0,  6, 20, 18,  0,  6, 20,  7,  0,  6, 20, 12,  0,  6, 20,  9,\n",
       "        0,  6, 20, 19,  0,  6, 20, 13,  0,  6, 20, 10,  0, 18, 20,  1,  0,\n",
       "       18, 20,  6,  0, 18, 20,  7,  0, 18, 20, 12,  0, 18, 20,  9,  0, 18,\n",
       "       20, 19,  0, 18, 20, 13,  0, 18, 20, 10,  0,  7, 20,  1,  0,  7, 20,\n",
       "        6,  0,  7, 20, 18,  0,  7, 20, 12,  0,  7, 20,  9,  0,  7, 20, 19,\n",
       "        0,  7, 20, 13,  0,  7, 20, 10,  0, 12, 20,  1,  0, 12, 20,  6,  0,\n",
       "       12, 20, 18,  0, 12, 20,  7,  0, 12, 20,  9,  0, 12, 20, 19,  0, 12,\n",
       "       20, 13,  0, 12, 20, 10,  0,  9, 20,  1,  0,  9, 20,  6,  0,  9, 20,\n",
       "       18,  0,  9, 20,  7,  0,  9, 20, 12,  0,  9, 20, 19,  0,  9, 20, 13,\n",
       "        0,  9, 20, 10,  0, 19, 20,  1,  0, 19, 20,  6,  0, 19, 20, 18,  0,\n",
       "       19, 20,  7,  0, 19, 20, 12,  0, 19, 20,  9,  0, 19, 20, 13,  0, 19,\n",
       "       20, 10,  0, 13, 20,  1,  0, 13, 20,  6,  0, 13, 20, 18,  0, 13, 20,\n",
       "        7,  0, 13, 20, 12,  0, 13, 20,  9,  0, 13, 20, 19,  0, 13, 20, 10,\n",
       "        0, 10, 20,  1,  0, 10, 20,  6,  0, 10, 20, 18,  0, 10, 20,  7,  0,\n",
       "       10, 20, 12,  0, 10, 20,  9,  0, 10, 20, 19,  0, 10, 20, 13,  0,  8,\n",
       "       20, 11,  0,  8, 20,  3,  0,  8, 20,  4,  0,  8, 20,  5,  0,  8, 20,\n",
       "       16,  0, 11, 20,  8,  0, 11, 20,  3,  0, 11, 20,  4,  0, 11, 20,  5,\n",
       "        0, 11, 20, 16,  0,  3, 20,  8,  0,  3, 20, 11,  0,  3, 20,  4,  0,\n",
       "        3, 20,  5,  0,  3, 20, 16,  0,  4, 20,  8,  0,  4, 20, 11,  0,  4,\n",
       "       20,  3,  0,  4, 20,  5,  0,  4, 20, 16,  0,  5, 20,  8,  0,  5, 20,\n",
       "       11,  0,  5, 20,  3,  0,  5, 20,  4,  0,  5, 20, 16,  0, 16, 20,  8,\n",
       "        0, 16, 20, 11,  0, 16, 20,  3,  0, 16, 20,  4,  0, 16, 20,  5,  0,\n",
       "        2, 20, 17,  0,  2, 20, 14,  0,  2, 20, 15,  0, 17, 20,  2,  0, 17,\n",
       "       20, 14,  0, 17, 20, 15,  0, 14, 20,  2,  0, 14, 20, 17,  0, 14, 20,\n",
       "       15,  0, 15, 20,  2,  0, 15, 20, 17,  0, 15, 20, 14,  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3cfbb7ae5da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# Use 1 so we don't have to rebuild model for generating data after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mbuffer_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m dataset_tf = (tf.data.Dataset.from_tensor_slices(dataset_enc) # Make tf dataset from encoded dataset\n\u001b[0m\u001b[1;32m     11\u001b[0m               \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Take a batch of 5 words at a time, dropping any remainder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_input_target\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# From each 5-word batch, return input (words 1-4) and target (words 2-5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Input and target are sets of 4 words, with target shifted one word into the future\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "seq_length = 4 # Length of input and target strings\n",
    "batch_size = 1 # Use 1 so we don't have to rebuild model for generating data after training\n",
    "buffer_size = 4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_tf = (tf.data.Dataset.from_tensor_slices(dataset_enc) # Make tf dataset from encoded dataset\n",
    "              .batch(seq_length+1, drop_remainder=True) # Take a batch of 5 words at a time, dropping any remainder\n",
    "              .map(split_input_target) # From each 5-word batch, return input (words 1-4) and target (words 2-5)\n",
    "              .shuffle(buffer_size) # tf reads in buffer_size elements into memory and shuffles those elements\n",
    "              .batch(batch_size, drop_remainder=True)) # This is the batch size used for training\n",
    "\n",
    "for batch_num, (input_text, target_text) in enumerate(dataset_tf.take(3)):\n",
    "    print(f'Batch {batch_num}')\n",
    "    for batch_input, batch_target in zip(input_text, target_text):\n",
    "        print(f'Input: {batch_input}')\n",
    "        print(f'Target: {batch_target}')\n",
    "        print('Input Translated: ' + ' '.join(encoder.inverse_transform(batch_input)))\n",
    "        print('Target Translated: ' + ' '.join(encoder.inverse_transform(batch_target)))\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
